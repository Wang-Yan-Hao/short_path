{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kola/git-project/short_path/Search_2D/video/vdo_origin.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kola/git-project/short_path/Search_2D/video/vdo_origin.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(frame, (width, height))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kola/git-project/short_path/Search_2D/video/vdo_origin.ipynb#W0sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Use a pre-trained model to predict objects in the frame.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kola/git-project/short_path/Search_2D/video/vdo_origin.ipynb#W0sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(frame, imgsz\u001b[39m=\u001b[39;49m(height, width), verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kola/git-project/short_path/Search_2D/video/vdo_origin.ipynb#W0sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Process each detected object result.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kola/git-project/short_path/Search_2D/video/vdo_origin.ipynb#W0sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/miniconda3/envs/autopytorch/lib/python3.8/site-packages/ultralytics/engine/model.py:235\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m prompts \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39m'\u001b[39m\u001b[39mset_prompts\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 235\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/miniconda3/envs/autopytorch/lib/python3.8/site-packages/ultralytics/engine/predictor.py:194\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/autopytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 43\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autopytorch/lib/python3.8/site-packages/ultralytics/engine/predictor.py:249\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39m# Preprocess\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 249\u001b[0m     im \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(im0s)\n\u001b[1;32m    251\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/autopytorch/lib/python3.8/site-packages/ultralytics/engine/predictor.py:121\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    119\u001b[0m     im \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform(im))\n\u001b[1;32m    120\u001b[0m     im \u001b[39m=\u001b[39m im[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtranspose((\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))  \u001b[39m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     im \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mascontiguousarray(im)  \u001b[39m# contiguous\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     im \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(im)\n\u001b[1;32m    124\u001b[0m im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import heapq\n",
    "import math\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "# Function to calculate the new height while maintaining the aspect ratio\n",
    "def resize_wh(orig_w, orig_h):\n",
    "    # h, w, d = image.shape\n",
    "    imgsz = 1280 # image size of yolo default is 640\n",
    "    new_h = imgsz/orig_w * orig_h # new height with same ratio\n",
    "    w = imgsz\n",
    "    remainder = (new_h % 32) # YOLO need w,h that can divide by 32\n",
    "    if remainder > 32/2:\n",
    "        h = int(new_h - remainder + 32)\n",
    "    else:\n",
    "        h = int(new_h - remainder)\n",
    "    return (w,h)\n",
    "\n",
    "# Video input path\n",
    "vdo_path = 'testvid2.mov'\n",
    "vdo = cv2.VideoCapture(vdo_path)\n",
    "\n",
    "# YOLO model path\n",
    "# model_path = r'./runs/detect/train36-v8l/weights/best.pt'\n",
    "model_path = r\"/home/kola/git-project/short_path/Search_2D/video/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Calculate new width and height while maintaining the aspect ratio\n",
    "width = int(vdo.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(vdo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width, height = resize_wh(width, height)\n",
    "\n",
    "# Get frames per second (fps) of the video\n",
    "fps = vdo.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Create a VideoWriter to save the output video\n",
    "out = cv2.VideoWriter(filename='testoutput13.mp4', fourcc=cv2.VideoWriter_fourcc(*'mp4v'), fps=fps, frameSize=(width, height))\n",
    "\n",
    "start_node = (25,1250) #(1100, 300) #(10,10) #testvid2-(25,1250) #testvid4-(25,25)\n",
    "goal_node = (1700,25) #(600, 800) #(1500,1200) #testvid2-(1700,25) #testvid4-(950,1275)\n",
    "\n",
    "frame_count = 0\n",
    "timels = []  # List to store timing information\n",
    "\n",
    "# Check if the video is still open and can be read.\n",
    "while vdo.isOpened():\n",
    "\n",
    "    # ret: A boolean value that indicates whether the frame was successfully read\n",
    "    # frame: A NumPy array representing the image frame that was read from the video.\n",
    "    ret, frame = vdo.read()\n",
    "\n",
    "    if ret:\n",
    "        # --- Object Detection Section ---\n",
    "\n",
    "        t0_det = time.perf_counter()\n",
    "\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert image to RGB color\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "        # Use a pre-trained model to predict objects in the frame.\n",
    "        results = model.predict(frame, imgsz=(height, width), verbose=False)\n",
    "\n",
    "        # Process each detected object result.\n",
    "        for result in results:\n",
    "            box = result.boxes.xywh  # Extract the bounding box information.\n",
    "\n",
    "        # Measure the time taken for object detection.\n",
    "        t1_det = time.perf_counter()\n",
    "\n",
    "        if start_node == goal_node:\n",
    "            # Append timing information and visualize the result.\n",
    "            timels.append([frame_count, t1_det-t0_det, t1_map-t0_map, t1_astar-t0_astar])\n",
    "            \n",
    "            # Create an image with detected objects but without labels.\n",
    "            final_img = result.plot(labels=False)\n",
    "\n",
    "            # Draw circles around objects to represent obstacles.\n",
    "            for coor in box:\n",
    "                cv2.circle(final_img, (int(coor[0]), int(coor[1])), obstacle_radius, (0,0,255), thickness=3)\n",
    "                cv2.circle(final_img, (int(coor[0]), int(coor[1])), obstacle_radius+50, (255,0,0), thickness=2)\n",
    "\n",
    "            # Draw a path and a circle representing the current position.\n",
    "            for i in range(len(path)-1):\n",
    "                cv2.line(final_img, (path[i][1],path[i][0]), (path[i+1][1],path[i+1][0]), (0,255,0), thickness=5)\n",
    "\n",
    "            final_img = cv2.circle(final_img, current_coor, 20, color=(255, 16, 240),thickness=-1)\n",
    "\n",
    "            # Write the resulting image to an output file. \n",
    "            out.write(final_img)\n",
    "            # plt.imshow(cv2.circle(result.plot(labels=False), current_coor, 20, color=(255, 16, 240),thickness=-1))\n",
    "            # plt.show()\n",
    "            print('frame', frame_count)\n",
    "            print(start_node)\n",
    "            frame_count += 1\n",
    "            clear_output(wait=True)\n",
    "            continue\n",
    "\n",
    "        # --- Path Planning Section ---\n",
    "\n",
    "        # Update the path every 5 frames.\n",
    "        if frame_count % 5 == 0:\n",
    "            # map generation part\n",
    "            t0_map = time.perf_counter()\n",
    "\n",
    "            # Create obstacle and danger zones on the map.\n",
    "            map = np.ones((height, width), dtype=np.uint16)\n",
    "            obstacle_radius = 50\n",
    "            danger_radius = 50\n",
    "            thickness = -1\n",
    "            \n",
    "            # Create obstacle and danger zones on the map.\n",
    "            for i in range(danger_radius):\n",
    "                g_cost = ((obstacle_radius+50+i)*i)+1\n",
    "                for coor in box:\n",
    "                    cv2.circle(map, (int(coor[0]), int(coor[1])), obstacle_radius+danger_radius, (g_cost), thickness)\n",
    "                danger_radius -= 1\n",
    "            for coor in box:\n",
    "                cv2.circle(map, (int(coor[0]), int(coor[1])), obstacle_radius, 0, thickness)\n",
    "\n",
    "            # Measure the time taken for map generation.\n",
    "            t1_map = time.perf_counter()\n",
    "\n",
    "            t0_astar = time.perf_counter()\n",
    "\n",
    "            # Initialize lists and dictionaries for A* algorithm.\n",
    "            open_list = [(0, (start_node))]\n",
    "            close_list = []\n",
    "            cumulative_g_cost = {}\n",
    "            cumulative_g_cost[start_node] = map[start_node[0]][start_node[1]]\n",
    "            came_from = {}\n",
    "\n",
    "            # A* algorithm to find a path.\n",
    "            while open_list:\n",
    "                _, current = heapq.heappop(open_list)\n",
    "                close_list.append(current)\n",
    "                # print(current)\n",
    "\n",
    "                # early exit\n",
    "                if current == goal_node:\n",
    "                    # Reconstruct the path if the goal is reached.\n",
    "                    path = [current]\n",
    "                    while current in came_from:\n",
    "                        # print(current)\n",
    "                        current = came_from[current]\n",
    "                        path.append(current)\n",
    "                    path.reverse()\n",
    "                    # print(path)\n",
    "                    break\n",
    "\n",
    "                # get neighbor node\n",
    "                for step in [(0,-25), (0,25), (-25,0), (25,0), (25,25), (-25,25), (25,-25), (-25,-25)]: #(0,-1), (0,1), (-1,0), (1,0), (1,1), (-1,1), (1,-1), (-1,-1)\n",
    "                    neighbor = (current[0] + step[0], current[1] + step[1])\n",
    "\n",
    "                    # within the grid range\n",
    "                    if neighbor[0] < 0 or neighbor[0] >= (map.shape[0]) or neighbor[1] < 0 or neighbor[1] >= map.shape[1]:\n",
    "                        continue\n",
    "\n",
    "                    # not step on the obstacle\n",
    "                    if map[neighbor[0]][neighbor[1]] == 0:\n",
    "                        continue\n",
    "\n",
    "                    if neighbor in close_list:\n",
    "                        continue\n",
    "\n",
    "                    if neighbor in dict(open_list).values():\n",
    "                        temp_g = map[neighbor[0]][neighbor[1]] + cumulative_g_cost[current]\n",
    "                        if temp_g >= cumulative_g_cost[neighbor]:\n",
    "                            continue\n",
    "                    \n",
    "                    cumulative_g_cost[neighbor] = map[neighbor[0]][neighbor[1]] + cumulative_g_cost[current]\n",
    "                    g_cost = map[neighbor[0]][neighbor[1]]\n",
    "                    h_cost = math.sqrt(((neighbor[0] - goal_node[0])**2) + ((neighbor[1] - goal_node[1])**2)) # euclidean distance\n",
    "                    f_cost = g_cost + h_cost\n",
    "                    \n",
    "                    came_from[neighbor] = current\n",
    "                    heapq.heappush(open_list, (f_cost, neighbor))\n",
    "\n",
    "            t1_astar = time.perf_counter()\n",
    "\n",
    "            # Handle cases where no path is found or path variables are not defined.\n",
    "            try:\n",
    "                current_coor\n",
    "            except NameError:\n",
    "                current_coor = start_node\n",
    "            try:\n",
    "                path\n",
    "            except NameError:\n",
    "                path = current_coor\n",
    "\n",
    "            timels.append([frame_count, t1_det-t0_det, t1_map-t0_map, t1_astar-t0_astar])\n",
    "\n",
    "            # Update the current position and visualize the result.\n",
    "            current_coor = (path[0][1], path[0][0]) # (x, y)\n",
    "            final_img = result.plot(labels=False)\n",
    "\n",
    "            # Draw circles around objects to represent obstacles.\n",
    "            for coor in box:\n",
    "                cv2.circle(final_img, (int(coor[0]), int(coor[1])), obstacle_radius, (0,0,255), thickness=3)\n",
    "                cv2.circle(final_img, (int(coor[0]), int(coor[1])), obstacle_radius+50, (255,0,0), thickness=2)\n",
    "\n",
    "            # Draw the path and a circle representing the current position.\n",
    "            for i in range(len(path)-1):\n",
    "                cv2.line(final_img, (path[i][1],path[i][0]), (path[i+1][1],path[i+1][0]), (0,255,0), thickness=5)\n",
    "            final_img = cv2.circle(final_img, current_coor, 20, color=(255, 16, 240),thickness=-1)\n",
    "\n",
    "            # Write the resulting image to an output file and update variables.\n",
    "            out.write(final_img)\n",
    "            if len(path) >= 2:\n",
    "                start_node = path[1]\n",
    "\n",
    "\n",
    "            # plt.imshow(cv2.circle(result.plot(labels=False), current_coor, 20, color=(255, 16, 240),thickness=-1))\n",
    "            # plt.show()\n",
    "            print('frame', frame_count)\n",
    "            print(path)\n",
    "            frame_count += 1\n",
    "            clear_output(wait=True)\n",
    "            continue\n",
    "\n",
    "        timels.append([frame_count, t1_det-t0_det, t1_map-t0_map, t1_astar-t0_astar])\n",
    "\n",
    "        # Create an image with detected objects but without labels.\n",
    "        final_img = result.plot(labels=False)\n",
    "\n",
    "        # Draw circles around objects to represent obstacles.\n",
    "        for coor in box:\n",
    "            cv2.circle(final_img, (int(coor[0]), int(coor[1])), obstacle_radius, (0,0,255), thickness=3)\n",
    "            cv2.circle(final_img, (int(coor[0]), int(coor[1])), obstacle_radius+50, (255,0,0), thickness=2)\n",
    "\n",
    "        # Draw the path and a circle representing the current position.     \n",
    "        for i in range(len(path)-1):\n",
    "            cv2.line(final_img, (path[i][1],path[i][0]), (path[i+1][1],path[i+1][0]), (0,255,0), thickness=5)\n",
    "        final_img = cv2.circle(final_img, current_coor, 20, color=(255, 16, 240),thickness=-1)\n",
    "\n",
    "        # Write the resulting image to an output file.\n",
    "        out.write(final_img)\n",
    "        # plt.imshow(cv2.circle(result.plot(labels=False), current_coor, 20, color=(255, 16, 240),thickness=-1))\n",
    "        # plt.show()\n",
    "\n",
    "        print('frame', frame_count)\n",
    "        print(path)\n",
    "        frame_count += 1\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the output file.\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>detection_time</th>\n",
       "      <th>map_generation_time</th>\n",
       "      <th>a_star_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>299.500000</td>\n",
       "      <td>0.053671</td>\n",
       "      <td>0.055990</td>\n",
       "      <td>0.003302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>173.349358</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.003752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052071</td>\n",
       "      <td>0.047518</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>0.052577</td>\n",
       "      <td>0.053376</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>299.500000</td>\n",
       "      <td>0.052774</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>0.001818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>449.250000</td>\n",
       "      <td>0.052963</td>\n",
       "      <td>0.057389</td>\n",
       "      <td>0.004974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>599.000000</td>\n",
       "      <td>0.547207</td>\n",
       "      <td>0.065038</td>\n",
       "      <td>0.015052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frame  detection_time  map_generation_time  a_star_time\n",
       "count  600.000000      600.000000           600.000000   600.000000\n",
       "mean   299.500000        0.053671             0.055990     0.003302\n",
       "std    173.349358        0.020188             0.004016     0.003752\n",
       "min      0.000000        0.052071             0.047518     0.000141\n",
       "25%    149.750000        0.052577             0.053376     0.000141\n",
       "50%    299.500000        0.052774             0.056631     0.001818\n",
       "75%    449.250000        0.052963             0.057389     0.004974\n",
       "max    599.000000        0.547207             0.065038     0.015052"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(timels)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(timels)\n",
    "df.rename(columns = {0:'frame', 1:'detection_time', 2:'map_generation_time', 3:'a_star_time'}, inplace = True)\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
